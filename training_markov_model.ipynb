{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__author__ = \"Zafar Hussain (University of Helsinki, IVVES Project)\"\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import ntpath\n",
    "from collections import defaultdict\n",
    "from uuid import UUID\n",
    "from functools import reduce\n",
    "import os\n",
    "from random import randrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "import bokeh.plotting as bpl\n",
    "import bokeh.transform as btr\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import colorcet as cc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from IPy import IP\n",
    "import socket\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITESPACE_QUOTES_REGEX = re.compile(r\"\"\"([^\\s]*?\\\".*?\\\"[^\\s]*?|[^\\s]*?\\'.*?\\'[^\\s]*?|\\'.*?\\'|\\\".*?\\\"|[^\\\"\\'\\s]+)\"\"\")\n",
    "SEMICOLON_REGEX = re.compile(r\"\"\"^[^\\\"']*;[^\\\"']*$\"\"\")\n",
    "\n",
    "FLAG_SEP_EQUALS_REGEX = re.compile(r\"\"\"^[-\\/]+[^\\=\\\"\\']+\\=(([^=]{1}.*)|())$\"\"\")\n",
    "FLAG_SEP_COLON_REGEX = re.compile(r\"\"\"^[\\/\\-][^\\:\\\"\\'\\=]+\\:[^=]*$\"\"\")\n",
    "\n",
    "REDIRECTION_LEFT_REGEX = re.compile(r\"\"\"(^[^'\"<>]+<[^'\"<>]+$)|(^<[^<>]+$)|(^[^<]+<$)\"\"\")\n",
    "DOUBLE_REDIRECTION_LEFT_REGEX = re.compile(r\"\"\"(^[^'\"<>]+<<[^'\"<>]+$)|(^<<[^<>]+$)|(^[^<]+<<$)\"\"\")\n",
    "REDIRECTION_RIGHT_REGEX = re.compile(r\"\"\"(^[^'\"<>]+>[^'\"<>]+$)|(^>[^>]+$)|(^[^<>]+>$)\"\"\")\n",
    "DOUBLE_REDIRECTION_RIGHT_REGEX = re.compile(r\"\"\"(^[^'\"<>]+>>[^'\"<>]+$)|(^>>[^>]+$)|(^[^<>]+>>$)\"\"\")\n",
    "\n",
    "PIPE_REGEX = re.compile(r\"\"\"(^[^'\"|]+\\|[^'\"|]+$)|(^\\|[^\\|]+$)|(^[^\\|]+\\|$)\"\"\")\n",
    "DOUBLE_PIPE_REGEX = re.compile(r\"\"\"(^[^'\"\\|]+\\|\\|[^'\"\\|]+$)|(^\\|\\|[^\\|]+$)|(^[^\\|]+\\|\\|$)\"\"\")\n",
    "\n",
    "test_re = re.compile(r'''\"(?=(?:[^\"]|'[^\"]'|\"[^\"^\"]*)*$)''')\n",
    "\n",
    "RE_CMD_LEX_LINUX = re.compile(\n",
    "    r\"\"\"\"((?:\\\\[\"\\\\]|[^\"])*)\"|'([^']*)'|(\\\\.)|(&&?|\\|\\|?|\\d?\\>|[<])|([^\\s'\"\\\\&|<>]+)|(\\s+)|(.)\"\"\"\n",
    ")\n",
    "RE_CMD_LEX_WINDOWS = re.compile(\n",
    "    r\"\"\"\"((?:\"\"|\\\\[\"\\\\]|[^\"])*)\"?()|(\\\\\\\\(?=\\\\*\")|\\\\\")|(&&?|\\|\\|?|\\d?>|[<])|([^\\s\"&|<>]+)|(\\s+)|(.)\"\"\"\n",
    ")\n",
    "\n",
    "EMPTY_STRING = \"<empty_string>\"\n",
    "\n",
    "\n",
    "class CmdlTokenizer:\n",
    "    \"\"\"Tokenizer that splits into command-lines into 'words' corresponding to their execution arguments.\n",
    "\n",
    "    Current implementation includes Windows and Linux OS tokenization logic in two \"modes\":\n",
    "    manual (basic, hard-coded) and custom (trying to resemble cmd.exe arg-parsing).\n",
    "\n",
    "    This class' main method is the 'tokenize' function, which splits a single command line\n",
    "    into a list of tokens as strings.\n",
    "\n",
    "    Attributes:\n",
    "        mode: identifies which logic is used to extract tokens, one of 'manual' or 'custom'\n",
    "        os_name: operating system which the command lines come from, one of 'windows' or 'linux'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"manual\", os_name=\"windows\"):\n",
    "        if mode not in [\"manual\", \"custom\"]:\n",
    "            raise AttributeError(f\"Wrong mode '{mode}'. Possible values: 'manual', 'custom'\")\n",
    "        self.mode = mode\n",
    "        if os_name.lower() not in [\"windows\", \"linux\"]:\n",
    "            raise AttributeError(\"Wrong os_name. Possible values: 'windows', 'linux'\")\n",
    "        self.os_name = os_name.lower()\n",
    "\n",
    "    def get_metadata(self):\n",
    "        metadata = {\"mode\": self.mode, \"os_name\": self.os_name}\n",
    "        if self.mode == \"custom\":\n",
    "            metadata[\"empty_string_tag\"] = EMPTY_STRING\n",
    "        return metadata\n",
    "\n",
    "    def _get_splitters(self, token):\n",
    "        \"\"\"Check if any splits apply to the input token and return a list of splitters if applicable.\n",
    "        This function's output should be passed to _the get_split_regex method of this class.\n",
    "\n",
    "        Splitters refer to:\n",
    "        - flags (options) with values, e.g. --python=python3.6 (equals or slash signs...)\n",
    "        - pipes and redirections\n",
    "\n",
    "        Splits logic may wary depending on the os_name and mode.\n",
    "\n",
    "        Args:\n",
    "            token (str): single token candidate to splitting into smaller pieces\n",
    "\n",
    "        Returns:\n",
    "            tuple (List[str], List[str]): one_char_splitters list followed by multichar_splitters list\n",
    "\n",
    "        \"\"\"\n",
    "        one_char_splitters = []\n",
    "        multichar_splitters = []\n",
    "\n",
    "        if (self.os_name == \"linux\" or (self.os_name == \"windows\" and self.mode == \"manual\")) and SEMICOLON_REGEX.match(\n",
    "            token\n",
    "        ):\n",
    "            one_char_splitters.append(\";\")\n",
    "        if FLAG_SEP_EQUALS_REGEX.match(token):\n",
    "            one_char_splitters.append(\"=\")\n",
    "        if FLAG_SEP_COLON_REGEX.match(token):\n",
    "            one_char_splitters.append(r\"\\:\")\n",
    "\n",
    "        if DOUBLE_REDIRECTION_LEFT_REGEX.match(token):\n",
    "            multichar_splitters.append(\"<<\")\n",
    "        elif REDIRECTION_LEFT_REGEX.match(token):\n",
    "            one_char_splitters.append(\"<\")\n",
    "\n",
    "        if DOUBLE_REDIRECTION_RIGHT_REGEX.match(token):\n",
    "            multichar_splitters.append(\">>\")\n",
    "        elif REDIRECTION_RIGHT_REGEX.match(token):\n",
    "            one_char_splitters.append(\">\")\n",
    "\n",
    "        if DOUBLE_PIPE_REGEX.match(token):\n",
    "            multichar_splitters.append(r\"\\|\\|\")\n",
    "        elif PIPE_REGEX.match(token):\n",
    "            one_char_splitters.append(r\"\\|\")\n",
    "        return one_char_splitters, multichar_splitters\n",
    "\n",
    "    def _get_split_regex(self, one_char_splitters, multichar_splitters):\n",
    "        splitters = []\n",
    "        if one_char_splitters:\n",
    "            splitters.append(f\"[{''.join(one_char_splitters)}]\")\n",
    "        if multichar_splitters:\n",
    "            splitters.extend(multichar_splitters)\n",
    "        if len(splitters) >= 1:\n",
    "            return \"(\" + \"|\".join(splitters) + \")\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _base_split(self, sentence, prog):\n",
    "     \n",
    "        if self.mode == \"manual\" and prog=='cmd.exe' and '\"\"' in sentence:\n",
    "            return test_re.split(sentence)\n",
    "        elif self.mode == \"manual\":\n",
    "            return WHITESPACE_QUOTES_REGEX.findall(sentence)\n",
    "        elif self.mode == \"custom\":\n",
    "            return self._custom_cmdline_split(sentence)\n",
    "        return []\n",
    "\n",
    "    def _custom_cmdline_split(self, s):\n",
    "        \"\"\"Multi-platform variant of shlex.split() for command-line splitting.\n",
    "        For use with subprocess, for argv injection etc. Using fast REGEX.\n",
    "        \"\"\"\n",
    "        args = []\n",
    "        accu = None  # collects pieces of one arg\n",
    "        if self.os_name == \"linux\":\n",
    "            re_cmd_lex = RE_CMD_LEX_LINUX\n",
    "        elif self.os_name == \"windows\":\n",
    "            re_cmd_lex = RE_CMD_LEX_WINDOWS\n",
    "        else:\n",
    "            return args\n",
    "\n",
    "        is_quote = False\n",
    "        for qs, qss, esc, pipe, word, white, fail in re_cmd_lex.findall(s):\n",
    "            if word:\n",
    "                pass  # most frequent\n",
    "            elif esc:\n",
    "                word = esc[1]\n",
    "            elif white or pipe:\n",
    "                if accu is not None:\n",
    "                    args.append(accu)\n",
    "                if pipe:\n",
    "                    args.append(pipe)\n",
    "                is_quote = False\n",
    "                accu = None\n",
    "                continue\n",
    "            elif fail:\n",
    "                raise ValueError(\"invalid or incomplete shell string\")\n",
    "            elif qs:\n",
    "                word = qs.replace('\\\\\"', '\"').replace(\"\\\\\\\\\", \"\\\\\")\n",
    "                if self.os_name == \"windows\":\n",
    "                    word = word.replace('\"\"', '\"')\n",
    "                is_quote = True\n",
    "            elif qss:\n",
    "                word = qss  # may be even empty; must be last\n",
    "            else:\n",
    "                if not is_quote:\n",
    "                    word = EMPTY_STRING\n",
    "            accu = (accu or \"\") + word\n",
    "        if accu is not None:\n",
    "            args.append(accu)\n",
    "        return args\n",
    "\n",
    "    def _merge_tokens(self, tokens : List[str]) -> List[str]:\n",
    "        \"\"\"Merge some tokens which are more useful glued together in a single token.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): list of tokens to be reviewed and merged if applicable\n",
    "\n",
    "        Returns:\n",
    "            List[str]: list of tokens after applying merge operations\n",
    "        \"\"\"\n",
    "\n",
    "        def _merge_ampersand(tokens: List[str]) -> List[str]:\n",
    "            \"\"\"Obtain redirection-related tokens similar to 2>&1 (Windows-specific).\"\"\"\n",
    "            found = False\n",
    "            for i in range(len(tokens) - 2):\n",
    "                if tokens[i] == \">\" and tokens[i + 1] == \"&\":\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                i += 1\n",
    "                replacement = tokens[i] + tokens[i + 1]\n",
    "                tokens[i] = replacement\n",
    "                for j in range(i + 1, len(tokens) - 1):\n",
    "                    tokens[j] = tokens[j + 1]\n",
    "                tokens = tokens[: len(tokens) - 1]\n",
    "            return tokens\n",
    "\n",
    "        def _merge_redirections(tokens:List[str]) -> List[str]:\n",
    "            \"\"\"Obtain redirection-related tokens such as >> (Windows-specific).\"\"\"\n",
    "            found = False\n",
    "            for i in range(len(tokens) - 1):\n",
    "                if tokens[i] == \">\" and tokens[i + 1] == \">\":\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                replacement = tokens[i] + tokens[i + 1]\n",
    "                tokens[i] = replacement\n",
    "                for j in range(i + 1, len(tokens) - 1):\n",
    "                    tokens[j] = tokens[j + 1]\n",
    "                tokens = tokens[: len(tokens) - 1]\n",
    "            return tokens\n",
    "\n",
    "        tokens = _merge_ampersand(tokens)\n",
    "        tokens = _merge_redirections(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, sentence: str, prog:str) -> List[str]:\n",
    "        \"\"\"Split command line into tokens.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): single command line input as string\n",
    "\n",
    "        Returns:\n",
    "            List[str]: list of tokens as strings\n",
    "        \"\"\"\n",
    "        raw_tokens: List[str] = []\n",
    "        for token in self._base_split(sentence, prog):\n",
    "            one_char_splitters, multichar_splitters = self._get_splitters(token)\n",
    "            split_regex = self._get_split_regex(one_char_splitters, multichar_splitters)\n",
    "            if split_regex:\n",
    "                token = re.split(\n",
    "                    split_regex,\n",
    "                    token,\n",
    "                    maxsplit=len(one_char_splitters) + len(multichar_splitters),\n",
    "                )\n",
    "                raw_tokens.extend(filter(None, token))\n",
    "            else:\n",
    "                raw_tokens.append(token)\n",
    "        raw_tokens = self._merge_tokens(raw_tokens)\n",
    "        return raw_tokens\n",
    "\n",
    "    \n",
    "    def tokenize_into_idxs(self, sentence:str) -> List[tuple]:\n",
    "        tokens = self.tokenize(sentence)\n",
    "        \n",
    "        idxs = []\n",
    "        pos = 0\n",
    "        for token in tokens:\n",
    "            token = token.replace(\"<empty_string>\", '\"\"')\n",
    "            istart = sentence.find(token, pos)\n",
    "            #if istart < 0:\n",
    "            #    raise Exception(f'Inconsistent tokenization for token=BEGIN{token}END of sentence BEGIN{sentence}END at pos={pos}.')\n",
    "            if istart < 0:\n",
    "                idxs.append((pos, len(sentence)))\n",
    "                pos = len(sentence)\n",
    "                break\n",
    "            else:\n",
    "                pos = istart+len(token)\n",
    "                idxs.append((istart, pos))\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing a tier-2 markov model\n",
    "def add2dict(dictionary, key, value):\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = []\n",
    "    dictionary[key].append(value)\n",
    "\n",
    "def list2probabilitydict(given_list):\n",
    "    probability_dict = {}\n",
    "    given_list_length = len(given_list)\n",
    "    for item in given_list:\n",
    "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
    "    for key, value in probability_dict.items():\n",
    "        probability_dict[key] = value / given_list_length\n",
    "    return probability_dict\n",
    "\n",
    "\n",
    "def train_markov_model(new_lines, program_name):\n",
    "    initial_word = {}\n",
    "    second_word = {}\n",
    "    transitions = {}\n",
    "    program_name = os.path.splitext(program_name)[0]+'.exe'\n",
    "    for line in new_lines:\n",
    "        tokens = tok_class.tokenize(line, program_name)\n",
    "        tokens = [x.strip() for x in tokens ]\n",
    "        tokens = [x.strip('\"') for x in tokens]\n",
    "        tokens = [x.replace('\\\\', ' \\\\')  for x in tokens if x]\n",
    "        tokens = [tok.replace(',','') for tok in tokens]\n",
    "        tokens = [tok.lower() for tok in tokens]\n",
    "        tokens = [x.split() for x in tokens]\n",
    "        tokens = [t for tok in tokens for t in tok]   \n",
    "        tokens = generate_N_grams(tokens, 1)\n",
    "        tokens = [tok.replace('\"','') for tok in tokens]\n",
    "        #tokens = [tok.split() for tok in tokens]\n",
    "        #tokens = [t for tok in tokens for t in tok]\n",
    "        #tokens = [tok.split('-') for tok in tokens]\n",
    "        #tokens = [t for tok in tokens for t in tok]\n",
    "        tokens_length = len(tokens)\n",
    "        for i in range(tokens_length):\n",
    "            token = tokens[i]#.replace('\"', '')\n",
    "            if i == 0:\n",
    "                #token = tokens[i].replace('\"', '')\n",
    "                initial_word[token] = initial_word.get(token, 0) + 1\n",
    "            else:\n",
    "                prev_token = tokens[i - 1].replace('\"', '')\n",
    "                #prev_token = prev_token.replace('\"', '')\n",
    "                if i == tokens_length - 1:\n",
    "                    add2dict(transitions, (prev_token, token), '<EOP>')\n",
    "                if i == 1: \n",
    "                    add2dict(second_word, prev_token, token)\n",
    "                else:\n",
    "                    prev_prev_token = tokens[i - 2].replace('\"', '')\n",
    "                    #prev_prev_token = prev_prev_token.replace('\"', '')\n",
    "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
    "    \n",
    "    # Normalize the distributions\n",
    "    initial_word_total = sum(initial_word.values())\n",
    "    for key, value in initial_word.items():\n",
    "        initial_word[key] = value / initial_word_total\n",
    "        \n",
    "    for prev_word, next_word_list in second_word.items():\n",
    "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
    "        \n",
    "    for word_pair, next_word_list in transitions.items():\n",
    "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
    "    \n",
    "\n",
    "    return initial_word, second_word, transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and mask UUID\n",
    "def mask_uuid(token):\n",
    "    uuid_regex = r\"\\{(.*?)\\}\"\n",
    "    if '{' in token and '}' in token:\n",
    "        matches = re.finditer(uuid_regex, token)\n",
    "        for matchNum, match in enumerate(matches):\n",
    "            for groupNum in range(0, len(match.groups())):\n",
    "                try:\n",
    "                    if match.group(1).lower()== str(UUID(match.group(1).lower())):   \n",
    "                        token = re.sub(r\"{([^{}]+)}\", r\"UUID\", token)\n",
    "                except ValueError:\n",
    "                        pass\n",
    "    elif '.' in token:\n",
    "        tokens = token.split('.')\n",
    "        try:\n",
    "            if UUID(tokens[0]):\n",
    "                token = 'RANDOM_TOKEN.'+tokens[1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            if UUID(token):\n",
    "                token = 'RANDOM_TOKEN'\n",
    "        except ValueError:\n",
    "            pass   \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect and mask numeric values\n",
    "def mask_files_int_floats(tokens):\n",
    "    for t in range(len(tokens)):\n",
    "        token = tokens[t]\n",
    "        tokens[t] = mask_uuid(token)\n",
    "        if token.startswith('\\\\') and token.endswith('.tmp'):\n",
    "            tokens[t] = '\\\\Random_Token'\n",
    "        elif token.endswith('.tmp'):\n",
    "            tokens[t] = 'Random_Token'\n",
    "        elif token.startswith('tmp') and len(token.split('.'))==2:\n",
    "            tokens[t] = 'Random_Token'\n",
    "        elif token.startswith('\\\\tmp') and len(token.split('.'))==2:\n",
    "            tokens[t] = '\\\\Random_Token'\n",
    "        elif 'cmdline' in token and token.split('.')[-1]=='cmdline':\n",
    "            tokens[t] = '\\\\Random_Token'\n",
    "        #elif token.startswith('\"') and token.endswith('\"') and token[1:-1].isdigit():\n",
    "        #    tokens[t] = 'integer_value'\n",
    "        elif token.isdigit():\n",
    "            tokens[t] = 'Random_Token'\n",
    "        elif token.replace('.','',1).isdigit() and token.count('.') < 2:\n",
    "            tokens[t]='Random_Token'\n",
    "        elif validate_iso8601(token):\n",
    "            tokens[t] = 'Random_Token'\n",
    "        elif contains_letters_in_order(token, 'RDS'):\n",
    "            tokens[t] = mask_rds(token)\n",
    "        elif token.startswith('\\\\rds') or token.startswith('rds') or token.startswith('\\\\RDS') or token.startswith('RDS'):\n",
    "            tokens[t] = mask_rds(token) \n",
    "        elif re.search(\"^[a-zA-Z0-9_]*$\", token) and tokens[t-1]=='-event':\n",
    "            tokens[t] = 'Random_Token'\n",
    "        elif token.startswith('\\\\upi') and tokens[t+1][2:].isalnum()  and tokens[t+2][2:].isalnum():\n",
    "            tokens[t+1]='\\\\Random_Token'\n",
    "            tokens[t+2]='\\\\Random_Token'\n",
    "        elif token== '-sha256' and tokens[t+1].isalnum() and len(tokens[t+1])==64:\n",
    "            tokens[t+1]= 'Random_Token'\n",
    "        elif token== '-token' and tokens[t+1].isalnum() and len(tokens[t+1])==40:\n",
    "            tokens[t+1]= 'Random_Token'\n",
    "        elif token =='-ip':\n",
    "            try:\n",
    "                socket.inet_aton(tokens[t+1])\n",
    "                tokens[t+1]='Random_Token'\n",
    "            except socket.error:\n",
    "                pass\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and mask RDS and date-time tokens\n",
    "def contains_letters_in_order(word, letters):\n",
    "    regex = '.*'.join(map(re.escape, letters))\n",
    "    return re.search(regex, word) is not None\n",
    "\n",
    "def mask_rds(token):\n",
    "    tt = token\n",
    "    if token.startswith('\\\\'):\n",
    "        tt = token.split('\\\\')\n",
    "        for v in range(len(tt)):\n",
    "            if tt[v].startswith('RDS') or tt[v].startswith('rds'):\n",
    "                vv = tt[v].split('.')\n",
    "                if len(vv[0])==15:\n",
    "                    vv[0]='Random_Token'\n",
    "                    tt[v] = '.'.join(vv)\n",
    "            if v<len(tt)-1:\n",
    "                tt[v]=tt[v]+'\\\\'\n",
    "            else:\n",
    "                tt[v]=tt[v] \n",
    "        tt = ''.join(tt)\n",
    "    elif token.startswith('\\\\rds') and len(token==15) or token.startswith('\\\\RDS') and len(token==15):\n",
    "        tt = 'Random_Token'  \n",
    "    elif token.startswith('rds'):\n",
    "        tt='Random_Token'\n",
    "    else:\n",
    "        pass\n",
    "    return tt\n",
    "\n",
    "def validate_iso8601(str_val):\n",
    "    date_time_regex = r'^(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])T(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(\\.[0-9]+)?(Z|[+-](?:2[0-3]|[01][0-9]):[0-5][0-9])?$'\n",
    "    date_time_regex2 = r'^(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])t(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(\\.[0-9]+)?(z|[+-](?:2[0-3]|[01][0-9]):[0-5][0-9])?$'\n",
    "    match_date_time = re.compile(date_time_regex).match\n",
    "    match_date_time2 = re.compile(date_time_regex2).match\n",
    "    try:            \n",
    "        if match_date_time( str_val ) is not None or match_date_time2( str_val ):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram=1): \n",
    "    temp=zip(*[text[i:] for i in range(0,ngram)])\n",
    "    ans=[', '.join(ngram) for ngram in temp]\n",
    "    return ans\n",
    "def detect_nested_commands(tokens, initial_word, second_word, transitions):\n",
    "    scores_index = []\n",
    "    last_index = len(tokens)\n",
    "    #print(tokens)\n",
    "    for i in range(len(tokens)): \n",
    "        #print(tokens[i]) \n",
    "        if i==0 and tokens[i] in initial_word:\n",
    "            #print(tokens[i], initial_word[string_cmd])\n",
    "            scores_index.append(initial_word[tokens[i]])\n",
    "        elif i==0 and tokens[i] not in initial_word:\n",
    "            scores_index.append(0)\n",
    "        elif i==1 and tokens[i-1] in second_word.keys():\n",
    "            next_word = second_word[tokens[i-1]]\n",
    "            new_cmd = tokens[i]\n",
    "            if new_cmd in next_word.keys():\n",
    "                scores_index.append(next_word[new_cmd])\n",
    "            else:\n",
    "                scores_index.append(0)\n",
    "        elif i==1 and tokens[i-1] not in second_word.keys():\n",
    "            scores_index.append(0)\n",
    "        elif i>1:\n",
    "            string_cmd = tokens[i-2]\n",
    "            next_dict = transitions[(string_cmd, new_cmd)]\n",
    "            if tokens[i] in next_dict:\n",
    "                #print(tokens[i], next_dict[tokens[i]])\n",
    "                scores_index.append(next_dict[tokens[i]])\n",
    "                new_cmd  =  tokens[i]\n",
    "            else:\n",
    "                #print('command not found in model, so the prob is: 0.000001')\n",
    "                last_index = i\n",
    "                break\n",
    "    return tokens, scores_index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect random directories\n",
    "def detect_random_dirs(commands, initial_word, second_word, transitions, random_number_prob, number_dir_prob, alnum_dir_prob, prog_name):\n",
    "    random_tokens = []\n",
    "    for j in range(len(commands)):\n",
    "        tokens = tok_class.tokenize(commands[j], prog_name+'.exe')\n",
    "        tokens = [x.strip() for x in tokens ]\n",
    "        tokens = [tok.replace('\"','') for tok in tokens]\n",
    "        tokens = [x.replace('\\\\', ' \\\\')  for x in tokens if x]\n",
    "        tokens = [tok.replace(',','') for tok in tokens]   \n",
    "        tokens = [tok.lower() for tok in tokens]\n",
    "        tokens = [x.split() for x in tokens]\n",
    "        tokens = [t for tok in tokens for t in tok]\n",
    "        tokens = generate_N_grams(tokens, 1)\n",
    "        tokens = [tok.replace('\"','') for tok in tokens]\n",
    "        tokens, prob = detect_nested_commands(tokens, initial_word, second_word, transitions)\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].isdigit() and prob[i]<random_number_prob: #0.00005\n",
    "                tokens[i]= 'Random_Token'\n",
    "            elif tokens[i].startswith('\\\\') and tokens[i][1:].isdigit() and prob[i]<number_dir_prob: #0.00009\n",
    "                tokens[i]= '\\\\Random_Token'\n",
    "            elif tokens[i].startswith('\\\\') and tokens[i][1:].isalnum() and prob[i]<alnum_dir_prob: #0.009\n",
    "                tokens[i]= '\\\\Random_Token'\n",
    "        random_tokens.append([' '.join( tokens)])\n",
    "    for r in range(len(random_tokens)):\n",
    "        random_tokens[r] = [random_tok.replace('monitoring host temporary files randomnumber','TempDir') for random_tok in random_tokens[r]]\n",
    "    return random_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokens and their probabilities\n",
    "def command_tokens_prob(command,  initial_word, second_word, transitions, prog_name):\n",
    "    tokens = tok_class.tokenize(command, prog_name+'.exe')\n",
    "    tokens = [x.strip() for x in tokens ]\n",
    "    tokens = [tok.replace('\"','') for tok in tokens]\n",
    "    tokens = [x.replace('\\\\', ' \\\\')  for x in tokens if x]\n",
    "    tokens = [tok.replace(',','') for tok in tokens]   \n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    tokens = [x.split() for x in tokens]\n",
    "    tokens = [t for tok in tokens for t in tok]\n",
    "    tokens = generate_N_grams(tokens, 1)\n",
    "    tokens = [tok.replace('\"','') for tok in tokens]\n",
    "    tokens, prob = detect_nested_commands(tokens, initial_word, second_word, transitions)\n",
    "    return tokens, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_original_files(new_lines, prog_name):\n",
    "    masked_lines = []\n",
    "    for p in range(len(new_lines)):\n",
    "        tokens = tok_class.tokenize(new_lines[p], prog_name+'.exe')\n",
    "        tokens = [x.strip() for x in tokens if x.strip()]\n",
    "        tokens = mask_files_int_floats(tokens)\n",
    "        masked_lines.append(' '.join(tokens))\n",
    "    return masked_lines\n",
    "    \n",
    "def detect_label_model(new_lines, prog_name, random_number_prob, number_dir_prob, alnum_dir_prob):\n",
    "    initial_word, second_word, transitions = train_markov_model(new_lines, prog_name)\n",
    "    random_number_detected_commands = detect_random_dirs(new_lines, initial_word, second_word, transitions, random_number_prob, number_dir_prob, alnum_dir_prob, prog_name)\n",
    "    random_number_detected_commands = [ran for rand in random_number_detected_commands for ran in rand]\n",
    "    initial_word, second_word, transitions = train_markov_model(random_number_detected_commands, prog_name)\n",
    "    tem_dir_detected_commands = detect_random_dirs(random_number_detected_commands, initial_word, second_word, transitions, random_number_prob, number_dir_prob, alnum_dir_prob, prog_name)\n",
    "    tem_dir_detected_commands = [ran for rand in tem_dir_detected_commands for ran in rand]   \n",
    "    return tem_dir_detected_commands\n",
    "    \n",
    "def command_tokens_freq(command, prog_name):\n",
    "    tok_class = CmdlTokenizer(os_name = 'windows', mode='manual')\n",
    "    tokens = tok_class.tokenize(command, prog_name+'.exe')\n",
    "    tokens = [x.strip() for x in tokens ]\n",
    "    tokens = [tok.replace('\"','') for tok in tokens]\n",
    "    tokens = [x.replace('\\\\', ' \\\\')  for x in tokens if x]\n",
    "    tokens = [tok.replace(',','') for tok in tokens]   \n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    tokens = [x.split() for x in tokens]\n",
    "    tokens = [t for tok in tokens for t in tok]\n",
    "    tokens = generate_N_grams(tokens, 1)\n",
    "    return tokens\n",
    "\n",
    "def avg_freq_dist(new_lines, prog_name):    \n",
    "    command_tokens = []\n",
    "    for p in range(len(new_lines)):\n",
    "        command_tokens.append(command_tokens_freq(new_lines[p], prog_name))\n",
    "    command_tokens = [word for w in command_tokens for word in w]\n",
    "    text = command_tokens#\"This is an example . This is test . example is for freq dist .\"\n",
    "    fd = FreqDist([word for word in text])\n",
    "    total = fd.N()\n",
    "    for word in fd:\n",
    "        fd[word] /= float(total)\n",
    "    all_fdist = pd.Series(dict(fd))\n",
    "    avg_threshold = all_fdist.mean()\n",
    "    max_threshold = all_fdist.max()\n",
    "    min_threshold = all_fdist.min()\n",
    "    return np.mean([np.sqrt(avg_threshold),np.sqrt(max_threshold),np.sqrt(min_threshold)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_silhouette_coeff(new_lines, prog_name, avg_threshold): \n",
    "    temp_dir_detected_commands=detect_label_model(new_lines,prog_name+'.exe',avg_threshold,avg_threshold,avg_threshold)\n",
    "    #print('Training successful.')\n",
    "    masked_lines = mask_original_files(temp_dir_detected_commands, prog_name)\n",
    "    return masked_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prog_name, os_name, and mode values can be changed\n",
    "tok_class = CmdlTokenizer(os_name = 'windows', mode='manual')\n",
    "prog_name = 'cmd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the text (commands)\n",
    "places = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('windows_commands.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        places.append(currentPlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the threshold value based on the data and mask the random tokens\n",
    "thr = avg_freq_dist(places, prog_name)\n",
    "masked_data = threshold_silhouette_coeff(places, prog_name, thr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
